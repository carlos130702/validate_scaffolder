from flask import Flask, request, jsonify
import pandas as pd
import os
import sys
import re
from pyhocon import ConfigFactory
import xmltodict
import requests
import numpy as np
import ast
import fnmatch
from langdetect import detect, DetectorFactory
from wordfreq import zipf_frequency

DetectorFactory.seed = 0

app = Flask(__name__)

def connection_to_bitbucket(target_url, token):
    status = False

    # Usar el token que viene del frontend
    if not token:
        print("[!] Error: No se proporcion√≥ token")
        return False, None

    # 2. Usa el token obtenido en la cabecera
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
        "Content-Type": "application/json",
        "Accept": "application/json",
        "Authorization": f"Bearer {token}",
        "X-Atlassian-Token": "nocheck"
    }

    try:
        response = requests.get(target_url, headers=headers)
        if response.status_code == 200:
            status = True
        else:
            print(f"[!] Error de conexi√≥n: C√≥digo de estado {response.status_code}")
            print(f"[!] Respuesta: {response.text}")
    except requests.exceptions.RequestException as e:
        print(f"[!] Error de red: {e}")
        return False, None

    return status, response


def get_files_of_repository(url_bitbucket, rama, token):
    name_branch = rama
    all_files_list = []
    name_branch_x = "develop" if name_branch.strip() == "" else name_branch.strip()
    
    if check_format_url(url_bitbucket):
        name_project = url_bitbucket.split("/")[-4]
        name_repository = url_bitbucket.split("/")[-2]
        target_api = "https://bitbucket.globaldevtools.bbva.com/bitbucket/rest/api/latest/projects/{}/repos/{}/files?at=refs%2Fheads%2F{}&start=0&limit=1000000".format(
            name_project, name_repository, name_branch_x
        )

        status, response = connection_to_bitbucket(target_api, token)
        if status:
            all_files_list = response.json()['values']
    else:
        print("[!] The URL does not comply with the specified format.")

    return all_files_list

def read_content_file_of_repository(url_bitbucket, filename, name_branch, token):
    content_file = ""
    name_branch_x = "develop" if name_branch.strip() == "" else name_branch.strip()

    name_project = url_bitbucket.split("/")[-4]
    name_repository = url_bitbucket.split("/")[-2]
    target_api = "https://bitbucket.globaldevtools.bbva.com/bitbucket/rest/api/latest/projects/{}/repos/{}/raw/{}?at=refs%2Fheads%2F{}".format(
        name_project, name_repository, filename, name_branch_x
    )
    
    status, response = connection_to_bitbucket(target_api, token)
    if status:
        content_file = response.text
        
    return content_file

def check_format_url(url_bitbucket):
    pattern_url = r"https:\/\/bitbucket\.globaldevtools\.bbva\.com\/bitbucket\/projects\/[\w-]+\/repos\/[\w-]+\/browse"
    if re.match(pattern_url, url_bitbucket):
        return True
    else:
        return False

# Lista de verbos v√°lidos para nombres de funciones
VALID_VERBS = [
    "add", "calculate", "group", "select", "join", "write", "sort",
    "drop", "extract", "check", "append", "is", "concat",
    "filter", "map", "merge", "split", "normalize", "transform",
    "validate", "load", "save", "fetch", "update", "remove",
    "create", "delete", "parse", "build", "clean", "format",
    "union", "run", "read","get", "filter", "cast", "recover", "replace"
]

# Funciones que deben ignorarse en la validaci√≥n
IGNORED_FUNCS = [
    "setUp", "tearDown", "main", "__init__", "__str__", "__repr__",
    "setUpClass", "tearDownClass"
]

# Patrones de archivos donde se debe validar
VALIDATION_PATTERNS = [
    r"^transformations.*\.py$",   # transformations*.py
    r"^test_transformations.*\.py$",  # test_transformations*.py
    r"^utils\.py$",               # utils.py
    r"^app\.py$"                  # app.py
]

def validate_function_names(file_content, filename):
    bad_funcs = []
    is_test_file = filename.startswith("test_") or "/test" in filename or "\\test" in filename

    try:
        tree = ast.parse(file_content, filename=filename)
    except SyntaxError:
        return bad_funcs

    functions = [node for node in ast.walk(tree) if isinstance(node, ast.FunctionDef)]

    # Ignorar la primera funci√≥n en archivos de test
    if is_test_file and functions:
        functions = functions[1:]

    for node in functions:
        func_name = node.name
        if func_name in IGNORED_FUNCS:
            continue

        parts = func_name.split("_")
        prefix = parts[0]
        func_invalid = False

        if is_test_file:
            # Tests deben empezar con test_ y luego un verbo v√°lido
            if not func_name.startswith("test_"):
                func_invalid = True
            else:
                if len(parts) > 1:
                    verb = parts[1]
                    if verb not in VALID_VERBS:
                        func_invalid = True
                else:
                    func_invalid = True
        else:
            # Verificar si el prefijo es un verbo v√°lido
            if prefix not in VALID_VERBS:
                func_invalid = True

            # Reglas especiales SOLO para estos verbos
            if prefix == "join" and "_with_" not in func_name:
                func_invalid = True
            if prefix == "group" and "_by_" not in func_name:
                func_invalid = True
            if prefix == "filter" and "_by_" not in func_name:
                func_invalid = True
            if prefix == "union" and "_with_" not in func_name:
                func_invalid = True

            # Nota: "run" se permite sin reglas especiales

        if func_invalid:
            bad_funcs.append(func_name)

    return bad_funcs

def check_function_verbs(repository_files, url_repository, rama, token):
    report_blocks = []

    for f in sorted(repository_files, key=lambda x: x.split("/")[-1]):
        fname = f.split("/")[-1]
        if any(re.match(pat, fname) for pat in VALIDATION_PATTERNS):
            content = read_content_file_of_repository(url_repository, f, rama, token)
            bad_funcs = validate_function_names(content, f)

            if bad_funcs:
                bad_funcs = sorted(bad_funcs)
                lines = [f"{i}. {func}" for i, func in enumerate(bad_funcs, 1)]
                block = f"{fname}:\n    " + "\n    ".join(lines)
                report_blocks.append(block)

    return "OK" if not report_blocks else "\n".join(report_blocks)

UDF_IMPORT_PATTERNS = [
    (r"\bfrom\s+pyspark\.sql\.functions\s+import\s+udf\b", "from pyspark.sql.functions import udf"),
    (r"\bpyspark\.sql\.functions\.udf\b", "pyspark.sql.functions.udf"),
    (r"\bimport\s+pyspark\.sql\.functions\s+as\s+\w+\b", "import pyspark.sql.functions as <alias>"),
    (r"\bfrom\s+pyspark\.sql\.types\s+import\s+UserDefinedType\b", "from pyspark.sql.types import UserDefinedType"),
]

def validate_no_udf(file_content, filename):
    hits = []
    lines = file_content.split("\n")

    for line in lines:
        clean = line.strip()
        if clean.startswith("#"):
            continue

        for regex, label in UDF_IMPORT_PATTERNS:
            if re.search(regex, clean):
                hits.append(label)

    if not hits:
        return "OK"

    # armar bloque numerado
    enumerated = [f"{i}. {h}" for i, h in enumerate(hits, 1)]
    return f"{filename}:\n    " + "\n    ".join(enumerated)

def check_no_udf(repository_files, url_repository, rama,token):
    target_patterns = [
        r"^transformations.*\.py$",
        r"^utils\.py$"
    ]

    reports = []
    any_hit = False

    for f in repository_files:
        fname = f.split("/")[-1]
        if any(re.match(pat, fname) for pat in target_patterns):
            content = read_content_file_of_repository(url_repository, f, rama, token)
            result = validate_no_udf(content, fname)
            if result != "OK":
                reports.append(result)
                any_hit = True

    return "OK" if not any_hit else "\n\n".join(reports).strip()

FIELDS_SUFFIX = "_FIELDS"
FIELD_SUFFIX = "_FIELD"

def extract_constants_from_code(code: str):
    constants = {}
    try:
        tree = ast.parse(code)
    except Exception as e:
        print(f"[!] Error parseando fields.py: {e}")
        return constants

    for node in tree.body:
        if isinstance(node, ast.Assign):
            for target in node.targets:
                if isinstance(target, ast.Name) and target.id.isupper():
                    try:
                        constants[target.id] = ast.literal_eval(node.value)
                    except (ValueError, SyntaxError):
                        # Si no se puede evaluar, guardar el tipo
                        if isinstance(node.value, ast.Str):
                            constants[target.id] = node.value.s
                        elif isinstance(node.value, ast.Dict):
                            constants[target.id] = {}
                        elif isinstance(node.value, ast.List):
                            constants[target.id] = []
                        elif isinstance(node.value, ast.Num):
                            constants[target.id] = node.value.n
                        elif isinstance(node.value, ast.NameConstant):
                            constants[target.id] = node.value.value
                        else:
                            constants[target.id] = None
        elif isinstance(node, ast.ClassDef):
            for stmt in node.body:
                if isinstance(stmt, ast.Assign):
                    for target in stmt.targets:
                        if isinstance(target, ast.Name) and target.id.isupper():
                            try:
                                constants[target.id] = ast.literal_eval(stmt.value)
                            except (ValueError, SyntaxError):
                                constants[target.id] = None
    return constants

def should_be_plural(value):
    if value is None:
        return False

    # Diccionarios siempre son _FIELDS
    if isinstance(value, dict):
        return True

    if isinstance(value, (list, tuple, set)) and len(value) > 1:
        return True

    if isinstance(value, (str, int, float, bool)) or value is None:
        return False

    if isinstance(value, (list, tuple)) and len(value) == 1:
        return False

    return False

def compute_correct_name(name, value):
    should_be_fields = should_be_plural(value)

    # Si ya tiene el sufijo correcto, no cambiar
    if should_be_fields and name.endswith(FIELDS_SUFFIX):
        return name
    elif not should_be_fields and name.endswith(FIELD_SUFFIX):
        return name

    # Remover sufijos incorrectos existentes
    if name.endswith(FIELDS_SUFFIX):
        base_name = name[:-len(FIELDS_SUFFIX)]
    elif name.endswith(FIELD_SUFFIX):
        base_name = name[:-len(FIELD_SUFFIX)]
    else:
        base_name = name

    # Agregar sufijo correcto
    if should_be_fields:
        return f"{base_name}{FIELDS_SUFFIX}"
    else:
        return f"{base_name}{FIELD_SUFFIX}"

def validate_field_naming(constants):
    errors = []

    for name, value in constants.items():
        current_suffix = None
        if name.endswith(FIELDS_SUFFIX):
            current_suffix = "FIELDS"
        elif name.endswith(FIELD_SUFFIX):
            current_suffix = "FIELD"
        else:
            # No tiene sufijo - error
            correct_name = compute_correct_name(name, value)
            errors.append(f"{name} ‚Üí {correct_name} (falta sufijo)")
            continue

        # Verificar si el sufijo actual es correcto
        should_be_fields = should_be_plural(value)
        correct_suffix = "FIELDS" if should_be_fields else "FIELD"

        if current_suffix != correct_suffix:
            correct_name = compute_correct_name(name, value)
            value_type = type(value).__name__
            if isinstance(value, dict):
                value_info = f"diccionario con {len(value)} elementos"
            elif isinstance(value, (list, tuple, set)):
                value_info = f"colecci√≥n con {len(value)} elementos"
            else:
                value_info = f"{value_type}: {str(value)[:50]}{'...' if len(str(value)) > 50 else ''}"

            errors.append(f"{name} ‚Üí {correct_name} ({value_info})")

    return errors

def validate_fields_field(url_repo, branch,token):
    # Buscar archivos
    files = get_files_of_repository(url_repo, branch,token)
    fields_candidates = [f for f in files if f.endswith("fields.py")]

    if not fields_candidates:
        return "‚ö†Ô∏è No se encontr√≥ fields.py en el repo"

    # Leer contenido
    fields_path = fields_candidates[0]
    content = read_content_file_of_repository(url_repo, fields_path, branch, token)
    constants = extract_constants_from_code(content)

    if not constants:
        return "‚ÑπÔ∏è No se encontraron constantes en fields.py"

    errors = validate_field_naming(constants)

    if not errors:
        return "‚úÖ OK"
    else:
        result = f"‚ùå Se encontraron {len(errors)} errores en la convenci√≥n FIELD/FIELDS:\n"
        result += "\n".join([f"  ‚Ä¢ {error}" for error in errors])
        return result

# Funci√≥n adicional para an√°lisis detallado (opcional)
def analyze_fields_structure(url_repo, branch):
    files = get_files_of_repository(url_repo, branch, token)
    fields_candidates = [f for f in files if f.endswith("fields.py")]

    if not fields_candidates:
        return "‚ö†Ô∏è No se encontr√≥ fields.py en el repo"

    fields_path = fields_candidates[0]
    content = read_content_file_of_repository(url_repo, fields_path, branch, token)
    constants = extract_constants_from_code(content)

    analysis = []
    analysis.append(f"üìä An√°lisis de {fields_path}:")
    analysis.append(f"Total de constantes: {len(constants)}")

    field_count = 0
    fields_count = 0
    no_suffix_count = 0

    for name, value in constants.items():
        if name.endswith(FIELD_SUFFIX):
            field_count += 1
        elif name.endswith(FIELDS_SUFFIX):
            fields_count += 1
        else:
            no_suffix_count += 1

    analysis.append(f"Constantes con _FIELD: {field_count}")
    analysis.append(f"Constantes con _FIELDS: {fields_count}")
    analysis.append(f"Constantes sin sufijo: {no_suffix_count}")

    # Validaci√≥n
    errors = validate_field_naming(constants)
    if errors:
        analysis.append(f"\n‚ùå Errores encontrados: {len(errors)}")
        for error in errors[:10]:  # Mostrar solo los primeros 10
            analysis.append(f"  ‚Ä¢ {error}")
        if len(errors) > 10:
            analysis.append(f"  ... y {len(errors) - 10} m√°s")
    else:
        analysis.append("\n‚úÖ Todas las constantes siguen la convenci√≥n FIELD/FIELDS")

    return "\n".join(analysis)

def validate_versions(file_kaafile_content, file_setup_cfg_content, file_setup_py_content, file_init_py_content):

    versions = {}

    # setup.cfg
    try:
        #La versi√≥n est√° en la segunda l√≠nea
        line = file_setup_cfg_content.splitlines()[1]
        match = re.search(r'=\s*(\S+)', line)
        if match:
            versions['setup.cfg'] = match.group(1).strip()
    except (IndexError, AttributeError):
        versions['setup.cfg'] = "No se encontr√≥ la versi√≥n"

    #setup.py
    try:
        match = re.search(r'version=["\'](.*?)["\']', file_setup_py_content)
        if match:
            versions['setup.py'] = match.group(1).strip()
    except (AttributeError):
        versions['setup.py'] = "No se encontr√≥ la versi√≥n"

    # __init__.py
    try:
        match = re.search(r'__version__\s*=\s*["\'](.*?)["\']', file_init_py_content)
        if match:
            versions['__init__.py'] = match.group(1).strip()
    except (AttributeError):
        versions['__init__.py'] = "No se encontr√≥ la versi√≥n"

    #Kaafile
    try:
        match = re.search(r'version\s*=\s*["\'](.*?)["\']', file_kaafile_content)
        if match:
            versions['Kaafile'] = match.group(1).strip()
    except (AttributeError):
        versions['Kaafile'] = "No se encontr√≥ la versi√≥n"

    #Comparar las versiones
    version_values = list(versions.values())
    # Eliminar versiones no encontradas
    version_values = [v for v in version_values if "No se encontr√≥" not in v]

    if len(set(version_values)) <= 1 and len(version_values) > 0:
        result = "Vesiones iguales:\n"
        for file, ver in versions.items():
            result += f"- {file}: {ver}\n"
        return result
    else:
        result = "Versiones no consistentes:\n"
        for file, ver in versions.items():
            result += f"- {file}: {ver}\n"
        return result

def validate_no_pandas(content, ruta_archivo):

    nombre_archivo = os.path.basename(ruta_archivo)

    usa_pandas = False
    if re.search(r'^\s*import\s+pandas', content, re.MULTILINE):
        usa_pandas = True
    elif re.search(r'^\s*from\s+pandas', content, re.MULTILINE):
        usa_pandas = True

    return {
        "Archivo": nombre_archivo,
        "Usa pandas": "S√≠" if usa_pandas else "No"
    }

def get_variables_short(file_transformations):

    nodo = ast.parse(file_transformations)
    variables = []

    for n in ast.walk(nodo):
        if isinstance(n, ast.Assign):  # solo asignaciones normales
            for target in n.targets:
                if isinstance(target, ast.Name):  # variable simple
                    variables.append(target.id.lower())

                elif isinstance(target, (ast.Tuple, ast.List)):  # asignaci√≥n m√∫ltiple
                    for e in target.elts:
                        if isinstance(e, ast.Name):
                            variables.append(e.id.lower())


    return variables

def busqueda_logica(file_app):
    palabra_1, palabra_2, palabra_3 = "if ", "for ","else "
    if palabra_1 in file_app or palabra_2 in file_app or palabra_3 in file_app:
        resultado = True
    else:
        resultado = False
    return resultado

def busqueda_spark(file):
    palabra = "spark.read."
    if palabra in file:
        resultado = True
    else:
        resultado = False
    return resultado

def busqueda_spark_write(file):
    palabra = ".write."
    if palabra in file:
        # If ".write." is found, now check for "dataproc_mock.write"
        if "dataproc_mock.write" in file:
            return False # Found "dataproc_mock.write", so result is False
        else:
            resultado = True
    else:
        # If ".write." is not found at all
        resultado = False
    return resultado

def busqueda_hadoop(file):
    palabra = "apache.hadoop"
    if palabra in file:
        resultado = True
    else:
        resultado = False
    return resultado

def lecturas_hasta_la_particion(file,file_conf):
    palabra_3 = "="
    file_new = file.replace(" = ","")
    file_new_2 = file_new.replace("==","")
    file_conf_2 = file_conf.replace(" = ","")

    if palabra_3 in file_conf_2:
        resultado = True
    elif palabra_3 in file_new_2:
        resultado = True
    else:
        resultado = False
    return resultado

def version_dataproc_sdk(file_requirements):
    target_version = "0.4.9"
    match = re.search(r"dataproc_sdk==([\d.]+)", file_requirements)
    if match:
        found_version = match.group(1)
        # Convert version strings to comparable tuples of integers
        found_version_parts = tuple(map(int, found_version.split('.')))
        target_version_parts = tuple(map(int, target_version.split('.')))
        if found_version_parts >= target_version_parts:
            resultado = True
        else:
            resultado = False
    else:
        # If dataproc_sdk is not found, consider it incorrect
        resultado = False
    return resultado

def compactacion(file):
    palabra_1 , palabra_2= ".coalesce(", "compact"
    if palabra_1 in file or palabra_2 in file:
        resultado = True
    else:
        resultado = False
    return resultado

def base_path(file):
    comilla_doble=chr(14)
    palabra_1, palabra_2, palabra_3 = ".option(" + comilla_doble + "basepath" + comilla_doble, "base_path","full_path"
    if palabra_1 in file or palabra_2 in file or palabra_3 in file:
        print('usa basepath')
        resultado = True
    else:
        resultado = False
    return resultado

def check_conf_size(file_conf_processed):
    try:
        start_index = file_conf_processed.find('{')
        if start_index != -1:
            relevant_content = file_conf_processed[start_index:]
            conf_char_count = len(relevant_content)
            if conf_char_count > 2400:
                return "Pas√≥ el l√≠mite", conf_char_count
            else:
                return "OK", conf_char_count
        else:
            return "application.conf size ERROR: No opening brace found", 0 # Handle cases where no '{' is found
    except Exception as e:
        return f"application.conf size ERROR: {e}", 0

def check_schema_value(file_conf):
    # Parse the HOCON configuration
    try:
        config = ConfigFactory.parse_string(file_conf)
    except Exception as e:
        return f"application.conf SCHEMA ERROR: Could not parse HOCON: {e}"

    # Search for keys containing "SCHEMA" and check their values
    for key, value in config.items():
        if "SCHEMA" in key.upper():
            if isinstance(value, str) and "gl-datio-da-generic-dev" in value:
                return "ERROR: Value contains gl-datio-da-generic-dev"
    return "OK"

def check_sonar_coverage(file_sonar):
    required_line = "sonar.python.coverage.reportPaths=coverage.xml"
    if required_line in file_sonar:
        return "OK"
    else:
        return "FALTA ACTUALIZAR EL SONARQUBE"

def check_input_output_suffixes(file_conf):
    input_found = False
    output_found = False
    # Split the file content into lines and iterate
    for line in file_conf.splitlines():
        # Check if the line contains a variable assignment (contains '=')
        if '=' in line:
            # Extract the variable name (part before the first '=')
            variable_name = line.split('=')[0].strip()
            # Check for suffixes
            if variable_name.upper().endswith('INPUT'):
                input_found = True
            if variable_name.upper().endswith('OUTPUT'):
                output_found = True
    if input_found and output_found:
        return "OK"
    else:
        return "Falta agregar los sufijos Input y Output a las variables"

def check_contributing_readme(repository_files):
    contributing_found = any("CONTRIBUTING" in file for file in repository_files)
    readme_found = any("README" in file for file in repository_files)

    if contributing_found and readme_found:
        return "OK"
    else:
        return "No se encuentran los archivos CONTRIBUTING y/o README"

# Empieza el filtro por idioma
def extract_docstring_params(docstring: str):
    if not docstring:
        return []
    return re.findall(r":param\s+([a-zA-Z_][a-zA-Z0-9_]*)\s*:", docstring)

whitelist = {
    "datetime", "utils", "Utils", "concat", "dataframe", "input", "output", "output_df", "filters", "epigraphs",
    "logger", "config", "partition", "result", "input_df", "StringType", "dataproc", "df", "AnalysisException",
    "StructType", "StructField", "relativedelta", "staticmethod", "joined_df", "date_str", "created_df", "DecimalType",
    "DataFrame", "dataframes", "Dataframe", "DoubleType", "persisted_dataframes", "aggregate_func", "unix", "timestamp", "catalog_df",
    "DateType", "irregula", "filtered_nulls_df", "tbuc", "disaggregate", "magicmock", "coment", "parquet", "cond", "guarannteect",
    "comment", "str", "df", "bussiness", "ci", "disaggregated", "input", "df", "fxin", "aggregate", "output", "fxin", "readed",
    "deposit", "emissions", "not", "nmd", "cts", "regexp", "replace", "sum", "amortization", "number", "param", "numerator",
    "pytest", "Constructor", "coalesce", "timedelta", "Dict", "SparkSession", "DataprocExperiment", "DatioPysparkSession",
    "function", "method", "class", "object", "variable", "parameter", "argument", "return", "returns", "value",
    "values", "data", "file", "files", "directory", "path", "paths", "name", "names", "type", "types", "code",
    "string", "strings", "number", "numbers", "integer", "float", "boolean", "array", "list", "lists", "dict",
    "dictionary", "set", "sets", "tuple", "tuples", "key", "keys", "value", "values", "index", "indexes", "item",
    "items", "element", "elements", "length", "size", "count", "total", "sum", "average", "minimum", "maximum",
    "first", "last", "next", "previous", "current", "new", "old", "original", "copy", "clone", "deep", "shallow",
    "null", "none", "true", "false", "yes", "no", "ok", "error", "errors", "exception", "exceptions", "warning",
    "warnings", "info", "information", "debug", "message", "messages", "log", "logs", "logging", "print", "output",
    "input", "source", "target", "destination", "result", "results", "output", "process", "processing", "compute",
    "calculation", "transform", "transformation", "convert", "conversion", "parse", "parsing", "validate", "validation",
    "check", "verify", "test", "testing", "unit", "integration", "system", "acceptance", "quality", "assurance",
    "development", "production", "staging", "testing", "environment", "configuration", "settings", "options",
    "parameters", "arguments", "default", "custom", "special", "general", "specific", "common", "unique", "different",
    "similar", "same", "equal", "equivalent", "identical", "distinct", "separate", "combined", "merged", "joined",
    "split", "divided", "separated", "connected", "linked", "related", "associated", "correlated", "mapped",
    "mapping", "relationship", "connection", "association", "correlation", "dependency", "dependent", "independent",
    "epigraph", "epigraphs", "perimeter", "hierarchies", "taxonomy", "segment", "exchange", "rate", "currency",
    "catalog", "identifier", "identifiers", "merges", "concatenated", "aliased", "substring", "predefined",
    "aliasing", "partitioned", "filtered", "joined", "added", "column", "values", "dict", "hierarchies",
    "offices", "experiment", "session", "mock", "dataproc", "pyspark", "spark", "dataframe", "dataframes"
}

def is_english_word(word: str, min_freq: float = 2.5) -> bool:
    word = word.lower().strip()

    # Verificar whitelist primero
    if word in whitelist:
        return True

    # Ignorar formatos de fecha y c√≥digos
    if re.match(r'^yyyy-MM-dd', word) or re.match(r'^[A-Za-z0-9_]+$', word) and len(word) <= 2:
        return True

    # Ignorar palabras que parecen ser nombres de variables t√©cnicas
    if '_' in word and word.replace('_', '').isalnum():
        return True

    # Verificar frecuencia
    try:
        freq = zipf_frequency(word, 'en')
        return freq >= min_freq
    except:
        # Si hay error en wordfreq, asumir que es ingl√©s
        return True

def is_english_text(text: str) -> bool:
    if not text:
        return True

    # 1. Reemplazar caracteres especiales por espacio
    clean_text = re.sub(r'[_:\.\-,;\*\\/]', ' ', text)

    # 2. Tokenizar
    words = [w.strip() for w in clean_text.split() if w.strip()]

    if not words:
        return True

    # 3. Validar cada palabra con criterios m√°s flexibles
    for word in words:
        word_lower = word.lower()

        # Ignorar palabras muy cortas
        if len(word) <= 2:
            continue

        # Ignorar formatos t√©cnicos
        if (re.match(r'^[A-Z][a-z]+[A-Z]', word) or  # CamelCase
            re.match(r'^[a-z_]+$', word) or          # snake_case
            re.match(r'^[A-Z_]+$', word)):           # CONSTANT_CASE
            continue

        # Verificar si es ingl√©s
        if not is_english_word(word):
            return False

    return True

def validate_functions_in_english(file_content: str, file_path) -> str:
    errors = []
    try:
        tree = ast.parse(file_content)
    except Exception as e:
        return f"ERROR analizando archivo: {e}"

    for node in ast.walk(tree):
        if isinstance(node, ast.FunctionDef):
            func_name = node.name
            docstring = ast.get_docstring(node)

            if docstring:
                lines = docstring.split('\n')
                for i, line in enumerate(lines):
                    line = line.strip()
                    if (line and 
                        not line.startswith(':') and 
                        not line.startswith('>>>') and 
                        not line.startswith('...') and  
                        not line.startswith('#') and     
                        len(line) > 10):             
                        
                        if not is_english_text(line):
                            words = line.split()
                            english_words = sum(1 for word in words if is_english_word(word))
                            if english_words < len(words) * 0.6:  # menos del 60% en ingl√©s
                                errors.append(f"documentaci√≥n l√≠nea {i+1}: '{line}'")

            func_params = [arg.arg for arg in node.args.args if arg.arg not in ("self", "cls")]
            for arg in func_params:
                technical_patterns = [
                    arg.endswith('_df'), arg.endswith('_mock'), arg.endswith('_config'),
                    arg.endswith('_dict'), arg.endswith('_list'), arg.endswith('_str'),
                    arg.endswith('_path'), arg.endswith('_file'), arg.endswith('_dir'),
                    '_perimeter_' in arg, '_hierarchies_' in arg, '_epigraph_' in arg,
                    '_exchange_' in arg, '_currency_' in arg, '_catalog_' in arg,
                    '_taxonomy_' in arg, '_rate_' in arg, '_filter_' in arg,
                    arg.startswith('df_'), arg.startswith('mock_'), arg.startswith('config_'),
                    len(arg) <= 3,  
                    arg.replace('_', '').isalnum() and len(arg) > 8 
                ]
                
                if any(technical_patterns):
                    continue  
                    
                if not is_english_text(arg):
                    errors.append(f"par√°metro: {arg}")

            local_vars = set()
            for subnode in ast.walk(node):
                if isinstance(subnode, ast.Assign):
                    for target in subnode.targets:
                        if isinstance(target, ast.Name):
                            var_name = target.id
                            technical_vars = [
                                'df', 'df1', 'df2', 'df3', 'result', 'temp', 'tmp',
                                'i', 'j', 'k', 'v', 'x', 'y', 'z', 'n', 'idx',
                                'col', 'row', 'val', 'item', 'elem', 'obj',
                                'lst', 'dct', 'arr', 'res', 'out', 'input',
                                'config', 'logger', 'spark', 'session', 'mock'
                            ]
                            if (var_name not in technical_vars and 
                                len(var_name) > 4 and  # solo variables significativas
                                not any(var_name.endswith(suffix) for suffix in ['_df', '_mock', '_dict', '_list', '_str']) and
                                not any(pattern in var_name for pattern in ['_perimeter_', '_hierarchies_', '_epigraph_'])):
                                local_vars.add(var_name)

            for var in local_vars:
                if not is_english_text(var):
                    parts = var.split('_')
                    if len(parts) > 1:
                        english_parts = sum(1 for part in parts if is_english_word(part) or len(part) <= 3)
                        if english_parts < len(parts) * 0.7:  
                            errors.append(f"variable: {var}")
                    else:
                        errors.append(f"variable: {var}")

            if ("tests/" not in file_path and 
                docstring and 
                len(docstring.strip()) > 50):  
                
                doc_params = extract_docstring_params(docstring)
                if len(func_params) > 0 and len(doc_params) > 0:
                    missing_in_doc = set(func_params) - set(doc_params)
                    extra_in_doc = set(doc_params) - set(func_params)
                    
                    if len(missing_in_doc) > len(func_params) * 0.5:  # m√°s del 50% faltan
                        errors.append(f"par√°metros no documentados: {', '.join(missing_in_doc)}")
                    if len(extra_in_doc) > 3:  # m√°s de 3 par√°metros extra
                        errors.append(f"par√°metros documentados pero no definidos: {', '.join(extra_in_doc)}")

    if errors:
        enumerated = [f"{idx}. {err}" for idx, err in enumerate(errors, start=1)]
        return "\n".join(enumerated)
    else:
        return "OK"

def validate_multiple_files_in_english(url_repository, rama, file_paths,token):
    results = []
    for file_path in file_paths:
        try:
            file_content = read_content_file_of_repository(url_repository, file_path, rama, token)

            validation_result = validate_functions_in_english(file_content, file_path)
            filename = os.path.basename(file_path)

            # ‚úÖ Solo agregar si hay errores reales (no "OK")
            if not validation_result.startswith("OK"):
                # ELIMINAR el filtro por n√∫mero de errores
                results.append(f"\n{filename}\n{validation_result}")

        except Exception as e:
            results.append(f"{os.path.basename(file_path)}\nError procesando archivo: {e}")

    return "\n".join(results) if results else "OK"

def check_commented_code(file_content, file_path):
    errors = []
    filename = os.path.basename(file_path)
    is_app_file = "app.py" in file_path
    
    for i, line in enumerate(file_content.splitlines(), start=1):
        stripped = line.strip()
        
        if (stripped.startswith("#") and 
            not stripped.startswith("#!") and 
            "coding" not in stripped.lower()):
            
            if is_app_file:
                if all(c in '# -=' for c in stripped): 
                    continue
                
                line_lower = stripped.lower()
                allowed_phrases = [
                    "your code starts here",
                    "your code ends here", 
                    "inicio del c√≥digo",
                    "fin del c√≥digo",
                    "start here",
                    "end here"
                ]
                if any(phrase in line_lower for phrase in allowed_phrases):
                    continue
            
            errors.append(f"L√≠nea {i}: {stripped[:80]}")

    if errors:
        enumerated = [f"{idx}. {err}" for idx, err in enumerate(errors, start=1)]
        return f"Comentarios invalidos {filename}:\n" + "\n".join(enumerated)
    else:
        return f"{filename} - ok"

def validate_commented_code_in_files(url_repository, rama, file_paths,token):
    results = []
    for file_path in file_paths:
        try:
            file_content = read_content_file_of_repository(url_repository, file_path, rama, token)

            comment_check = check_commented_code(file_content, file_path)
            results.append(comment_check)
        except:
            results.append(f"{file_path}\nArchivo no encontrado")

    return "\n".join(results)

def data_frame(url_repository, rama, token):
    repository_files = get_files_of_repository(url_repository, rama, token)
    name_project = url_repository.split("/")[-4]
    name_repository = url_repository.split("/")[-2]

    # files de inter√©s
    files_analysis = ["/utils/", "/transformations/", "/experiment.py", "/app.py"]
    files = []
    for file in repository_files:
        [files.append(file) for i in files_analysis if i in file]

    file_aplicattion = 'resources/application.conf'
    file_requirements = 'requirements.txt'
    file_app = name_repository+'/app.py'
    file_sonar_properties = 'sonar-project.properties'
    file_constants = name_repository+'/constants.py'
    file_utils = name_repository+'/utils/utils.py'
    file_fields = name_repository+'/fields.py'
    file_constants = name_repository+'/constants.py'
    file_test_utils = 'tests/utils/test_utils.py'
    file_test_app = 'tests/test_app.py'

    # ARCHIVOS PARA CHEQUEAR VERSIONES
    file_kaafile = 'Kaafile'
    file_setup_cfg = 'setup.cfg'
    file_setup_py = 'setup.py'
    file_init_py =  name_repository+'/__init__.py'

    # Variables de estado
    x1 = True
    x2 = True
    x3 = True
    x4 = True
    x5 = True
    x6 = True

    file_conf = read_content_file_of_repository(url_repository, file_aplicattion, rama, token)
    file_conf_processed = file_conf.replace('\n', '\r\n')

    # Procesar archivos para an√°lisis de Spark, Hadoop, etc.
    for fil in files:
        file = read_content_file_of_repository(url_repository, fil, rama, token)

       # Spark
        resultado_spark = busqueda_spark(file)
        if resultado_spark and x1:
            result_spark = "‚ùå Lectura con Spark"
            x1 = False
        elif not resultado_spark and x1:
            result_spark = "‚úÖ No se est√° usando lectura con Spark"

        # Hadoop
        resultado_hadoop = busqueda_hadoop(file)
        if resultado_hadoop and x2:
            result_hadoop = "‚ùå Uso de hadoop"
            x2 = False
        elif not resultado_hadoop and x2:
            result_hadoop = "‚úÖ No se est√° usando hadoop"

        # Lectura hasta la particion
        resultado_lect_part = lecturas_hasta_la_particion(file, file_conf)
        if resultado_lect_part and x3:
            result_lec_part = "‚úÖ Lectura hasta el Objeto"
            x3 = False
        elif not resultado_lect_part and x3:
            result_lec_part = "‚ùå Lectura hasta la particion"

        # Escritura con Spark
        resultado_spark_write = busqueda_spark_write(file)
        if resultado_spark_write and x4:
            result_write = "‚ùå Escritura con Spark"
            x4 = False
        elif not resultado_spark_write and x4:
            result_write = "‚úÖ No se est√° escribiendo con Spark"

        # Uso de Compactacion
        resultado_comp = compactacion(file)
        if resultado_comp and x5:
            result_compac = "‚úÖ Se esta usando compactacion"
            x5 = False
        elif not resultado_comp and x5:
            result_compac = "‚ùå No se esta usando compactacion"

        # Uso de base_path
        resultado_base_path = base_path(file)
        if resultado_base_path and x6:
            result_basepath = "‚ùå Se esta usando base_path"
            x6 = False
        elif not resultado_base_path and x6:
            result_basepath = "‚úÖ No se esta usando base_path"


   # Version correcta de dataproc_sdk
    file_requirements = read_content_file_of_repository(url_repository, file_requirements, rama, token)
    resultado_dataproc = version_dataproc_sdk(file_requirements)
    if resultado_dataproc:
        result_dataproc = "‚úÖ Version correcta de dataproc"
    else:
        result_dataproc = "‚ùå Version incorrecta de dataproc"

    # Uso de l√≥gica en app.py
    file_app = read_content_file_of_repository(url_repository, file_app, rama, token)
    resultado_busqlogicaapp = busqueda_logica(file_app)
    if resultado_busqlogicaapp:
        result_busqlogicaapp = "‚ùå S√≠ se est√° haciendo uso de l√≥gica (if, for, else)"
    else:
        result_busqlogicaapp = "‚úÖ No se detect√≥ l√≥gica compleja"

    files_transformations = [
        file for file in repository_files
        if f"{name_repository}/transformations/" in file and "transformations" in file.split("/")[-1]
    ]

    files_to_validate_main = files_transformations + [
        f"{name_repository}/utils/utils.py",
        f"{name_repository}/app.py",
        f"{name_repository}/fields.py",
        f"{name_repository}/constants.py",
    ]

    files_to_validate_tests = [
        "tests/transformations/test_transformations.py",
        "tests/utils/test_utils.py",
        "tests/test_app.py",
    ]

    files_to_validate = files_to_validate_main + files_to_validate_tests

    english_validation_general = validate_multiple_files_in_english(url_repository, rama, files_to_validate, token)
    resultado_pr = validate_commented_code_in_files(url_repository, rama, files_to_validate, token)
    verbos_output = check_function_verbs(repository_files, url_repository, rama, token)
    conf_check_field_fields = validate_fields_field(url_repository, rama, token)
    udf_output = check_no_udf(repository_files, url_repository, rama, token)

    file_transformations = [
        f for f in repository_files
        if fnmatch.fnmatch(f, "*/transformations/transformations*.py")
    ]

    file_test_transformations = [
        x for x in repository_files
        if fnmatch.fnmatch(x, "*tests/transformations/test_transformations*.py")
    ]

    file_paths_verb_ex = [
        file_utils,
        file_app,
        file_fields,
        file_constants,
        file_test_utils,
        file_test_app
    ]

    file_paths_verb = [*file_transformations, *file_test_transformations, *file_paths_verb_ex]

    ListVerb = [
        "add", "calculate", "group", "select", "join", "write", "sort",
        "drop", "extract", "check", "append", "is", "concat",
        "filter", "map", "merge", "split", "normalize", "transform",
        "validate", "load", "save", "fetch", "update", "remove",
        "create", "delete", "parse", "build", "clean", "format",
        "union", "run", "read","get", "filter", "cast", "recover", "replace"
    ]

    matches_by_file = {}
    for ruta_verb in file_paths_verb:
        content = read_content_file_of_repository(url_repository, ruta_verb, rama, token)
        file_name = os.path.basename(ruta_verb)
        variables = get_variables_short(content)
        variables_match = [var for var in variables if var.split("_")[0] in ListVerb]
        if variables_match:
            matches_by_file[file_name] = variables_match

    salida = ""
    for archivo, variables in matches_by_file.items():
        salida += f"\n{archivo}:\n"
        for idx, var in enumerate(variables, start=1):
            salida += f"  {idx}. {var}\n"

    # Validaci√≥n de Pandas
    file_paths_pandas = [*file_transformations, *file_test_transformations, file_utils]
    result_pandas_re = {}
    for ruta_pandas in file_paths_pandas:
        content = read_content_file_of_repository(url_repository, ruta_pandas, rama, token)
        result_pandas = validate_no_pandas(content, ruta_pandas)
        result_pandas_re[result_pandas["Archivo"]] = result_pandas["Usa pandas"]

    result_pandas_str = ""
    for archivo, estado in result_pandas_re.items():
        icon = "‚ùå" if estado == "S√≠" else "‚úÖ"
        result_pandas_str += f"{icon} {archivo}: {estado}\n"

    # Otras validaciones
    conf_size_check, conf_char_count = check_conf_size(file_conf_processed)
    schema_value_check = check_schema_value(file_conf)

    file_sonar = read_content_file_of_repository(url_repository, file_sonar_properties, rama, token)
    sonar_coverage_check = check_sonar_coverage(file_sonar)

    input_output_check = check_input_output_suffixes(file_conf)
    contributing_readme_check = check_contributing_readme(repository_files)

    file_constants_content = read_content_file_of_repository(url_repository, file_constants, rama, token)


    # Check versions
    file_kaafile_content = read_content_file_of_repository(url_repository, file_kaafile, rama, token)
    file_setup_cfg_content = read_content_file_of_repository(url_repository, file_setup_cfg, rama, token)
    file_setup_py_content = read_content_file_of_repository(url_repository, file_setup_py, rama, token)
    file_init_py_content = read_content_file_of_repository(url_repository, file_init_py, rama, token)
    resultado_validacion_versiones = validate_versions(file_kaafile_content, file_setup_cfg_content, file_setup_py_content, file_init_py_content)

     # ========== PRESENTACI√ìN MEJORADA ==========
    print("=" * 80)
    print(f"üìä AN√ÅLISIS DEL REPOSITORIO: {name_repository}")
    print("=" * 80)

    # Configuraci√≥n y Entorno
    print("\nüîß CONFIGURACI√ìN Y ENTORNO")
    print("-" * 40)
    print(f"üìñ Lectura de datos: {result_spark}")
    print(f"‚úçÔ∏è  Escritura de datos: {result_write}")
    print(f"üêò Uso de Hadoop: {result_hadoop}")
    print(f"üìä Nivel de lectura: {result_lec_part}")
    print(f"üîó Versi√≥n Dataproc SDK: {result_dataproc}")
    print(f"üóúÔ∏è  Compactaci√≥n: {result_compac}")
    print(f"üìÅ Base Path: {result_basepath}")
    print(f"üîç L√≥gica en app.py: {result_busqlogicaapp}")

    # Archivos de Configuraci√≥n
    print("\nüìÅ ARCHIVOS DE CONFIGURACI√ìN")
    print("-" * 40)
    icon_size = "‚úÖ" if "OK" in conf_size_check else "‚ùå"
    print(f"üìè Tama√±o application.conf: {icon_size} {conf_size_check} ({conf_char_count} caracteres)")

    icon_schema = "‚úÖ" if "OK" in schema_value_check else "‚ùå"
    print(f"üè∑Ô∏è  Schema productivo: {icon_schema} {schema_value_check}")

    icon_sonar = "‚úÖ" if "OK" in sonar_coverage_check else "‚ùå"
    print(f"üìä Configuraci√≥n Sonar: {icon_sonar} {sonar_coverage_check}")

    icon_io = "‚úÖ" if "OK" in input_output_check else "‚ùå"
    print(f"üî§ Sufijos Input/Output: {icon_io} {input_output_check}")

    icon_docs = "‚úÖ" if "OK" in contributing_readme_check else "‚ùå"
    print(f"üìö Documentaci√≥n: {icon_docs} {contributing_readme_check}")

    # Calidad de C√≥digo - MEJORADO
    print("\nüéØ CALIDAD DE C√ìDIGO")
    print("=" * 40)

    # Organizar en subsecciones
    print("\nüìù CONVENCIONES DE C√ìDIGO")
    print("-" * 30)

    # Verbos en funciones
    if verbos_output == "OK":
        print("üî§ Verbos en funciones: ‚úÖ Correctos")
    else:
        print("üî§ Verbos en funciones: ‚ùå Problemas detectados")
        if verbos_output != "OK":
            print("\n   üìã FUNCIONES IDENTIFICADAS:")
            print("   " + "=" * 50)
            
            # Procesar y formatear la salida
            lines = verbos_output.split('\n')
            current_file = ""
            
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                    
                # Detectar si es un archivo nuevo
                if line.endswith(':') or ':' in line and not any(char.isdigit() for char in line.split(':')[0]):
                    if ':' in line:
                        file_part = line.split(':')[0]
                        current_file = file_part.strip()
                        print(f"\n   üìÅ {current_file}:")
                    else:
                        current_file = line.replace(':', '').strip()
                        print(f"\n   üìÅ {current_file}:")
                        
                # Detectar l√≠neas con funciones numeradas
                elif any(char.isdigit() for char in line) and ('.' in line or line.startswith(('1', '2', '3', '4', '5', '6', '7', '8', '9'))):
                    # Separar las funciones numeradas
                    functions = []
                    current_func = ""
                    
                    # Procesar la l√≠nea para separar funciones
                    parts = line.split()
                    i = 0
                    while i < len(parts):
                        if parts[i].endswith('.') and parts[i][:-1].isdigit():
                            # Es un n√∫mero, empezar nueva funci√≥n
                            if current_func:
                                functions.append(current_func.strip())
                            current_func = parts[i] + " " + (parts[i+1] if i+1 < len(parts) else "")
                            i += 2
                        else:
                            current_func += " " + parts[i]
                            i += 1
                    
                    if current_func:
                        functions.append(current_func.strip())
                    
                    # Imprimir funciones formateadas
                    for func in functions:
                        if func.strip():
                            print(f"     {func}")
                            
                # Para l√≠neas que son claramente nombres de archivo sin ":"
                elif any(ext in line for ext in ['.py', '.py:']):
                    current_file = line.replace(':', '').strip()
                    print(f"\n   üìÅ {current_file}:")
                    
                else:
                    # L√≠nea normal de texto
                    print(f"     {line}")

    # Fields
    if conf_check_field_fields == "‚úÖ OK":
        print("üè∑Ô∏è  Fields/FIELDS: ‚úÖ Correctos")
    else:
        print("üè∑Ô∏è  Fields/FIELDS: ‚ùå Problemas detectados")
        if conf_check_field_fields != "‚úÖ OK":
            print("\n   ‚ö†Ô∏è  ERRORES ENCONTRADOS:")
            print("   " + "-" * 30)
            lines = conf_check_field_fields.split('\n')
            for line in lines:
                if line.strip():
                    if "‚Äì" in line:  # L√≠nea con formato FIELD ‚Äì FIELD
                        field_error = line.strip()
                        print(f"\n   üî∏ {field_error}")
                    elif "Se encontraron" in line or "errores" in line:
                        print(f"\n   üìä {line.strip()}")

    print("\nüåé IDIOMA Y DOCUMENTACI√ìN")
    print("-" * 30)

    # Idiomas
    if english_validation_general == "OK":
        print("üó£Ô∏è  Idioma: ‚úÖ Todo en ingl√©s")
    else:
        print("üó£Ô∏è  Idioma: ‚ùå Problemas detectados")
        if english_validation_general != "OK":
            for line in english_validation_general.split('\n'):
                if line.strip():
                    if not line.startswith(' ') and not line.startswith('1.'):
                        print(f"   üìÑ {line}")
                    else:
                        print(f"     {line}")

    print("\nüö´ RESTRICCIONES T√âCNICAS")
    print("-" * 30)

    # Uso de Pandas
    pandas_errors = sum(1 for line in result_pandas_str.split('\n') if "‚ùå" in line)
    if pandas_errors == 0:
        print("üêº Uso de Pandas: ‚úÖ Correcto")
    else:
        print(f"üêº Uso de Pandas: ‚ùå {pandas_errors} archivos usan Pandas")
        # Mostrar solo los archivos problem√°ticos
        for line in result_pandas_str.split('\n'):
            if "‚ùå" in line and line.strip():
                print(f"   {line}")

    # UDF
    if udf_output == "OK":
        print("‚ö° UDF: ‚úÖ No se detectaron UDFs")
    else:
        print("‚ö° UDF: ‚ùå Se detectaron UDFs")
        lines = udf_output.split('\n')
        for i, line in enumerate(lines[:3]):  # Mostrar solo primeros 3
            if line.strip():
                print(f"   {i+1}. {line}")

    comment_errors = len([line for line in resultado_pr.split('\n') if "Comentarios invalidos" in line])
    if comment_errors == 0:
        print("üí¨ C√≥digo comentado: ‚úÖ Correcto")
    else:
        print(f"üí¨ C√≥digo comentado: ‚ùå {comment_errors} archivos con problemas")
        for line in resultado_pr.split('\n'):
            if line.strip():
                if "Comentarios invalidos" in line:
                    print(f"   üìÑ {line}")
                elif line.startswith(('1.', '2.', '3.', '4.', '5.', '6.', '7.', '8.', '9.')):
                    print(f"     {line}")

    print("\nüî¢ CONTROL DE VERSIONES")
    print("-" * 30)

    version_lines = resultado_validacion_versiones.split('\n')
    if version_lines and "iguales" in version_lines[0].lower():
        print("üì¶ Versiones: ‚úÖ Consistentes")
        for line in version_lines[1:]:  # Mostrar detalles
            if line.strip() and line.startswith('-'):
                print(f"   {line}")
    else:
        print("üì¶ Versiones: ‚ùå Inconsistentes")
        for line in version_lines:
            if line.strip():
                print(f"   {line}")

    if salida.strip():
        print("\n‚ö†Ô∏è  VARIABLES CON PATR√ìN DE VERBOS")
        print("-" * 40)
        files_with_verbs = len([line for line in salida.split('\n') if line.endswith(':')])
        print(f"Se encontraron {files_with_verbs} archivos con variables que siguen patr√≥n de verbos")
        # MOSTRAR TODOS los archivos y variables
        for line in salida.split('\n'):
            if line.strip():
                if line.endswith(':'):
                    print(f"   üìÅ {line}")
                elif line.strip().startswith(('1.', '2.', '3.', '4.', '5.', '6.', '7.', '8.', '9.')):
                    print(f"     {line}")

    print("\n" + "=" * 80)
    print("‚úÖ AN√ÅLISIS COMPLETADO")
    print("=" * 80)

    d = {
        "engine": [name_repository],
        "Lectura": [result_spark],
        "Uso_Hadoop": [result_hadoop],
        "Lectura_Particion": [result_lec_part],
        "version_dataproc": [result_dataproc],
        "escritura": [result_write],
        "Uso de compactacion": [result_compac],
        "Uso base_path": [result_basepath],
        "Se hace uso de l√≥gica en el app.py?": [result_busqlogicaapp],
        "application.conf Caracteres": [conf_size_check],
        "Cant. Caracteres": [conf_char_count],
        "SCHEMA Productivo": [schema_value_check],
        "Archivo Sonar": [sonar_coverage_check],
        "Input/Output Var. Entorno": [input_output_check],
        "CONTRIBUTING/README": [contributing_readme_check],
        "Palabra con idioma inv√°lido": [english_validation_general],
        "Nombre de funci√≥n correcta": [verbos_output],
        "Field y Fields Incorrectos": [conf_check_field_fields],
        "Nombre de campo/variable incorrectos": [salida],
        "Uso de Pandas": [result_pandas_str],
        "Validaci√≥n UDF": [udf_output],
        "Codigo Comentado Inv√°lido (###)": [resultado_pr],
        "Resultado de validaci√≥n de versiones": [resultado_validacion_versiones]
    }

    return pd.DataFrame(data=d)


@app.route('/')
def index():
    return '''
    <!DOCTYPE html>
    <html lang="es">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>üöÄ Analizador de Repositorios Bitbucket</title>
        <style>
            * {
                margin: 0;
                padding: 0;
                box-sizing: border-box;
            }
            
            body {
                font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
                background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                min-height: 100vh;
                padding: 20px;
            }
            
            .container {
                max-width: 1000px;
                margin: 0 auto;
                background: white;
                border-radius: 15px;
                box-shadow: 0 20px 40px rgba(0,0,0,0.1);
                overflow: hidden;
            }
            
            .header {
                background: #2c3e50;
                color: white;
                padding: 30px;
                text-align: center;
            }
            
            .header h1 {
                font-size: 2.5em;
                margin-bottom: 10px;
            }
            
            .header p {
                font-size: 1.2em;
                opacity: 0.9;
            }
            
            .form-section {
                padding: 30px;
            }
            
            .form-group {
                margin-bottom: 25px;
            }
            
            label {
                display: block;
                margin-bottom: 8px;
                font-weight: 600;
                color: #2c3e50;
                font-size: 14px;
            }
            
            input, textarea {
                width: 100%;
                padding: 12px 15px;
                border: 2px solid #e9ecef;
                border-radius: 8px;
                font-size: 16px;
                transition: border-color 0.3s ease;
            }
            
            input:focus, textarea:focus {
                border-color: #4285f4;
                outline: none;
                box-shadow: 0 0 0 3px rgba(66, 133, 244, 0.1);
            }
            
            .button-group {
                display: flex;
                gap: 15px;
                flex-wrap: wrap;
                margin-top: 30px;
            }
            
            button {
                background: #4285f4;
                color: white;
                padding: 15px 30px;
                border: none;
                border-radius: 8px;
                cursor: pointer;
                font-size: 16px;
                font-weight: 600;
                transition: all 0.3s ease;
                display: flex;
                align-items: center;
                gap: 8px;
            }
            
            button:hover {
                background: #3367d6;
                transform: translateY(-2px);
            }
            
            button:disabled {
                background: #ccc;
                cursor: not-allowed;
                transform: none;
            }
            
            .btn-secondary {
                background: #6c757d;
            }
            
            .btn-secondary:hover {
                background: #545b62;
            }
            
            .loading {
                display: none;
                text-align: center;
                padding: 40px;
                background: #f8f9fa;
                border-radius: 10px;
                margin: 20px 0;
            }
            
            .spinner {
                border: 4px solid #f3f3f3;
                border-top: 4px solid #4285f4;
                border-radius: 50%;
                width: 40px;
                height: 40px;
                animation: spin 1s linear infinite;
                margin: 0 auto 20px;
            }
            
            @keyframes spin {
                0% { transform: rotate(0deg); }
                100% { transform: rotate(360deg); }
            }
            
            .resultado {
                display: none;
                margin-top: 30px;
                padding: 0;
                border-radius: 10px;
                border: 1px solid #e9ecef;
            }
            
            .result-header {
                background: #28a745;
                color: white;
                padding: 20px;
                display: flex;
                align-items: center;
                justify-content: space-between;
            }
            
            .result-content {
                padding: 25px;
                max-height: 600px;
                overflow-y: auto;
            }
            
            .section {
                margin-bottom: 25px;
                padding: 20px;
                background: #f8f9fa;
                border-radius: 8px;
                border-left: 4px solid #4285f4;
            }
            
            .section h3 {
                color: #2c3e50;
                margin-bottom: 15px;
                display: flex;
                align-items: center;
                gap: 10px;
            }
            
            .metric-card {
                background: white;
                padding: 15px;
                margin: 10px 0;
                border-radius: 6px;
                border: 1px solid #e9ecef;
            }
            
            .estado-ok { color: #28a745; font-weight: bold; }
            .estado-error { color: #dc3545; font-weight: bold; }
            .estado-warning { color: #ffc107; font-weight: bold; }
            
            .help-text {
                font-size: 12px;
                color: #6c757d;
                margin-top: 5px;
                font-style: italic;
            }
            
            .required {
                color: #dc3545;
            }
            
            @media (max-width: 768px) {
                .container {
                    margin: 10px;
                }
                
                .header h1 {
                    font-size: 2em;
                }
                
                .button-group {
                    flex-direction: column;
                }
                
                button {
                    width: 100%;
                }
            }
        </style>
    </head>
    <body>
        <div class="container">
            <div class="header">
                <h1>üöÄ Analizador de Repositorios Bitbucket</h1>
                <p>Analiza autom√°ticamente la calidad de c√≥digo de tus repositorios</p>
            </div>
            
            <div class="form-section">
                <div class="form-group">
                    <label for="url">URL del Repositorio Bitbucket <span class="required">*</span></label>
                    <input type="url" id="url" placeholder="https://bitbucket.globaldevtools.bbva.com/bitbucket/projects/..." required>
                    <div class="help-text">URL completa del repositorio de Bitbucket</div>
                </div>
                
                <div class="form-group">
                    <label for="rama">Rama a analizar <span class="required">*</span></label>
                    <input type="text" id="rama" placeholder="develop" value="develop" required>
                    <div class="help-text">Nombre de la rama que deseas analizar</div>
                </div>
                
                <div class="form-group">
                    <label for="token">Token de Bitbucket</label>
                    <input type="password" id="token" placeholder="Ingresa tu token si es necesario">
                </div>

                <div class="button-group">
                    <button onclick="analizar()" id="btnAnalizar">
                        <span>üîç</span> Ejecutar An√°lisis
                    </button>
                    <button onclick="limpiar()" class="btn-secondary">
                        <span>üîÑ</span> Limpiar
                    </button>
                </div>
            </div>

            <div id="loading" class="loading">
                <div class="spinner"></div>
                <h3>‚è≥ Analizando repositorio...</h3>
                <p>Esto puede tomar unos momentos mientras revisamos todos los archivos</p>
            </div>

            <div id="resultado" class="resultado">
                <!-- Los resultados se cargan aqu√≠ din√°micamente -->
            </div>
        </div>

        <script>
            function analizar() {
                const url = document.getElementById('url').value.trim();
                const rama = document.getElementById('rama').value.trim();
                const token = document.getElementById('token').value.trim();
                
                if (!url) {
                    alert('‚ùå Por favor ingresa una URL v√°lida');
                    return;
                }
                
                if (!rama) {
                    alert('‚ùå Por favor ingresa el nombre de la rama');
                    return;
                }
                
                // Mostrar loading
                document.getElementById('loading').style.display = 'block';
                document.getElementById('resultado').style.display = 'none';
                document.getElementById('btnAnalizar').disabled = true;
                
                // Enviar petici√≥n
                fetch('/analizar', {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json',
                    },
                    body: JSON.stringify({
                        url: url,
                        rama: rama,
                        token: token
                    })
                })
                .then(response => response.json())
                .then(data => {
                    mostrarResultado(data);
                })
                .catch(error => {
                    mostrarError(error);
                })
                .finally(() => {
                    document.getElementById('loading').style.display = 'none';
                    document.getElementById('btnAnalizar').disabled = false;
                });
            }
            
            function mostrarResultado(data) {
                const resultadoDiv = document.getElementById('resultado');
                
                if (data.success) {
                    let html = `
                        <div class="result-header">
                            <h2><span>‚úÖ</span> An√°lisis Completado</h2>
                            <span>${new Date().toLocaleString()}</span>
                        </div>
                        <div class="result-content">    
                    `;
                    
                    // Mostrar datos del an√°lisis
                    if (data.data && data.columns) {
                        html += '<div class="section"><h3>üìä Resultados del An√°lisis</h3>';
                        
                        data.columns.forEach((columna, index) => {
                            if (data.data[columna] && data.data[columna][0]) {
                                const valor = data.data[columna][0];
                                const estado = getEstadoFromValor(valor);
                                
                                html += `
                                    <div class="metric-card">
                                        <strong>${columna}:</strong>
                                        <span class="${estado.clase}">${valor}</span>
                                    </div>
                                `;
                            }
                        });
                        
                        html += '</div>';
                    }
                    
                    html += `</div></div>`;
                    resultadoDiv.innerHTML = html;
                } else {
                    mostrarError(data.error);
                }
                
                resultadoDiv.style.display = 'block';
            }
            
            function mostrarError(error) {
                const resultadoDiv = document.getElementById('resultado');
                const errorMsg = typeof error === 'string' ? error : error.message || 'Error desconocido';
                
                resultadoDiv.innerHTML = `
                    <div class="result-header" style="background: #dc3545;">
                        <h2><span>‚ùå</span> Error en el An√°lisis</h2>
                    </div>
                    <div class="result-content">
                        <div class="section">
                            <h3>‚ö†Ô∏è Detalles del Error</h3>
                            <div class="metric-card">
                                <strong>Error:</strong>
                                <span class="estado-error">${errorMsg}</span>
                            </div>
                            <p>Verifica que la URL sea correcta y que tengas acceso al repositorio.</p>
                        </div>
                    </div>
                `;
                resultadoDiv.style.display = 'block';
            }
            
            function limpiar() {
                document.getElementById('url').value = '';
                document.getElementById('rama').value = 'develop';
                document.getElementById('token').value = '';
                document.getElementById('resultado').style.display = 'none';
                document.getElementById('resultado').innerHTML = '';
            }
            
            function getEstadoFromValor(valor) {
                const texto = String(valor || ""); // fuerza a string incluso si es null/undefined
                if (texto.includes('‚úÖ') || texto.includes('OK')) {
                    return { clase: 'estado-ok' };
                } else if (texto.includes('‚ùå') || texto.includes('ERROR')) {
                    return { clase: 'estado-error' };
                } else if (texto.includes('‚ö†Ô∏è') || texto.includes('WARNING')) {
                    return { clase: 'estado-warning' };
                }
                return { clase: '' };
            }
            
            document.addEventListener('keypress', function(event) {
                if (event.key === 'Enter') {
                    analizar();
                }
            });
        </script>
    </body>
    </html>
    '''

# === API ENDPOINTS ===
@app.route('/analizar', methods=['POST'])
def analizar():
    try:
        data = request.get_json()
        url = data.get("url")
        rama = data.get("rama", "develop")
        token = data.get("token")
        
        if not url or not token:
            return jsonify({"success": False, "error": "URL y Token son requeridos"}), 400
        
        print(f"üîç Analizando: {url} (rama: {rama})")
        
        # Usar tu funci√≥n corregida
        resultado = data_frame(url, rama, token)
        
        # Convertir para el frontend
        if hasattr(resultado, 'to_dict'):
            df_dict = resultado.to_dict('list')
            response_data = {
                "success": True,
                "data": df_dict,
                "columns": list(resultado.columns),
                "url": url,
                "rama": rama
            }
        else:
            response_data = {
                "success": True,
                "data": {"resultado": str(resultado)},
                "url": url, 
                "rama": rama
            }
            
        return jsonify(response_data)
        
    except Exception as e:
        print(f"‚ùå Error: {str(e)}")
        return jsonify({"success": False, "error": str(e)}), 500

@app.route('/health', methods=['GET'])
def health():
    return jsonify({"status": "healthy"})

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=8080, debug=False)